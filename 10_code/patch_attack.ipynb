{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 23:15:04.292145: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-14 23:15:04.292202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-14 23:15:04.293443: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-14 23:15:04.301297: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-14 23:15:05.449650: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/workspaces/Adversarial-Patches-Experimentation/10_code/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from scipy.ndimage import rotate\n",
    "from pretrained_models.resnet20 import ResNetCIFAR\n",
    "\n",
    "from utils import *\n",
    "from importlib import reload\n",
    "reload(sys.modules['utils'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# preapre data\n",
    "\n",
    "# PARAMS\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "target = 1 #automobile\n",
    "\n",
    "#CIFAR-10 image tensor mean and std\n",
    "NORM_MEAN = [0.4914, 0.4822, 0.4465]\n",
    "NORM_STD = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "\n",
    "# TODO: should we apply the below transformations?\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN,\n",
    "                         std=NORM_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN,\n",
    "                         std=NORM_STD)\n",
    "])\n",
    "\n",
    "transform_image = transforms.ToPILImage()\n",
    "\n",
    "print('==> Preparing data..')\n",
    "trainset = torchvision.datasets.CIFAR10(root='../00_data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../00_data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_patch_circle(patch_size):\n",
    "    radius = patch_size//2\n",
    "    patch = np.zeros(( 3, radius*2, radius*2))    \n",
    "    for i in range(3):\n",
    "        a = np.zeros((radius*2, radius*2))    \n",
    "        cx, cy = radius, radius # The center of circle \n",
    "        y, x = np.ogrid[-radius: radius, -radius: radius]\n",
    "        index = x**2 + y**2 <= radius**2\n",
    "        a[cy-radius:cy+radius, cx-radius:cx+radius][index] = np.random.rand()\n",
    "        idx = np.flatnonzero((a == 0).all((1)))\n",
    "        a = np.delete(a, idx, axis=0)\n",
    "        patch[i] = np.delete(a, idx, axis=1)\n",
    "    return patch\n",
    "# init a circle path and visualize it\n",
    "patch = init_patch_circle(patch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR_MEANS, TENSOR_STD = torch.FloatTensor(NORM_MEAN)[:,None,None], torch.FloatTensor(NORM_STD)[:,None,None]\n",
    "def patch_forward(patch):\n",
    "    # Map patch values from [-infty,infty] to ImageNet min and max\n",
    "    patch = (torch.tanh(patch) + 1 - 2 * TENSOR_MEANS) / (2 * TENSOR_STD)\n",
    "    return patch\n",
    "\n",
    "\n",
    "def place_patch(img, patch):\n",
    "    \n",
    "    for i in range(img.shape[0]): # number of channels\n",
    "        # rot = np.random.choice(360)   \n",
    "        # patch = torch.from_numpy(rotate(patch.detach().numpy(), rot, axes = (1,2), reshape=False))\n",
    "        h_offset = np.random.randint(0,img.shape[2]-patch.shape[1]-1)\n",
    "        w_offset = np.random.randint(0,img.shape[3]-patch.shape[2]-1)\n",
    "        img[i,:,h_offset:h_offset+patch.shape[1],w_offset:w_offset+patch.shape[2]] = patch_forward(patch)\n",
    "    \n",
    "    return img # ,rot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_patch(model, patch, val_loader, target_class):\n",
    "    model.eval()\n",
    "    tp, tp_5, counter = 0., 0., 0.\n",
    "    n = 0 # number of images\n",
    "    with torch.no_grad():\n",
    "        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n",
    "            # For stability, place the patch at 4 random locations per image, and average the performance\n",
    "            for _ in range(4):\n",
    "                patch_img = place_patch(img, patch)\n",
    "                patch_img = patch_img.to(device)\n",
    "                img_labels = img_labels.to(device)\n",
    "                pred = model(patch_img)\n",
    "                # In the accuracy calculation, we need to exclude the images that are of our target class\n",
    "                # as we would not \"fool\" the model into predicting those\n",
    "                tp += torch.logical_and(pred.argmax(dim=-1) == target_class, img_labels != target_class).sum()\n",
    "                tp_5 += torch.logical_and((pred.topk(5, dim=-1)[1] == target_class).any(dim=-1), img_labels != target_class).sum()\n",
    "                counter += (img_labels != target_class).sum()\n",
    "                n += (img_labels != target_class).sum()\n",
    "    acc = tp/counter\n",
    "    top5 = tp_5/counter\n",
    "    attack_success_rate = tp/n\n",
    "    return acc, top5, attack_success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_attack_targeted(model, target_class, patch_size, num_epochs=5):\n",
    "    # Leave a small set of images out to check generalization\n",
    "    # In most of our experiments, the performance on the hold-out data points\n",
    "    # was as good as on the training set. Overfitting was little possible due\n",
    "    # to the small size of the patches.\n",
    "    train_set, val_set = torch.utils.data.random_split(trainset, [0.8, 0.2])\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last = True, num_workers=4)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "    # Create parameter and optimizer\n",
    "    # if not isinstance(patch_size, tuple):\n",
    "    #     patch_size = (patch_size, patch_size)\n",
    "    patch = nn.Parameter(torch.tensor(init_patch_circle(patch_size)), requires_grad= True)#nn.Parameter(torch.zeros(3, patch_size[0], patch_size[1]), requires_grad=True)\n",
    "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        t = tqdm(train_loader, leave=False)\n",
    "        for img, _ in t:\n",
    "            img = place_patch(img, patch)\n",
    "            img = img.to(device)\n",
    "            pred = model(img)\n",
    "            labels = torch.zeros(img.shape[0], device=pred.device, dtype=torch.long).fill_(target_class)\n",
    "            loss = loss_module(pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n",
    "        if epoch % 5 == 0:\n",
    "            acc, top5, attack_success_rate = eval_patch(model, patch, val_loader, target_class)\n",
    "            print(f\"Epoch {epoch}, Attack Success Rate: {attack_success_rate.item()}.\")\n",
    "\n",
    "    # Final validation\n",
    "    # acc, top5, attack_success_rate = eval_patch(model, patch, val_loader, target_class)\n",
    "\n",
    "    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item(), \"attack_success_rate\": attack_success_rate.item()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the Pre-Trained ResNet-20 model\n",
    "net = ResNetCIFAR(num_layers=20, Nbits=None)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(\"./pretrained_models/pretrained_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Attack Success Rate: 0.3535367548465729.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Attack Success Rate: 0.9400242567062378.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Attack Success Rate: 0.9476109147071838.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Attack Success Rate: 0.9540388584136963.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    }
   ],
   "source": [
    "p, x = patch_attack_targeted(net, target_class=1, patch_size=16, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "acc, top5, attack_success_rate = eval_patch(net, p, testloader, target_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untargeted Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_patch_untarget(model, patch, val_loader):\n",
    "    model.eval()\n",
    "    tp  = 0\n",
    "    n = 0 # number of images\n",
    "    with torch.no_grad():\n",
    "        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n",
    "            # For stability, place the patch at 4 random locations per image, and average the performance\n",
    "            for _ in range(4):\n",
    "                patch_img = place_patch(img, patch)\n",
    "                patch_img = patch_img.to(device)\n",
    "                img_labels = img_labels.to(device)\n",
    "                pred = model(patch_img)\n",
    "                # In the accuracy calculation, we need to exclude the images that are of our target class\n",
    "                # as we would not \"fool\" the model into predicting those\n",
    "                # get number of incorrect predictions\n",
    "                tp += (pred.argmax(dim=-1) != img_labels).sum()\n",
    "                n += img_labels.shape[0]\n",
    "\n",
    "    attack_success_rate = tp/n\n",
    "    return attack_success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_attack_untargeted(model, patch_size=16, num_epochs=5):\n",
    "    train_set, val_set = torch.utils.data.random_split(trainset, [0.8, 0.2])\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last = True, num_workers=4)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4)\n",
    "    \n",
    "    patch = nn.Parameter(torch.tensor(init_patch_circle(patch_size)), requires_grad= True)\n",
    "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        t = tqdm(train_loader, leave=False)\n",
    "        for img, labels in t:\n",
    "            img = place_patch(img, patch)\n",
    "            img = img.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model(img)\n",
    "            loss = - loss_module(pred, labels) # make it negative to maximize the loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n",
    "        if epoch % 5 == 0:\n",
    "            attack_success_rate = eval_patch_untarget(model, patch, val_loader)\n",
    "            print(f\"Epoch {epoch}, Attack Success Rate: {attack_success_rate.item()}.\")\n",
    "\n",
    "    # Final validation\n",
    "    attack_success_rate = eval_patch_untarget(model, patch, val_loader)\n",
    "    print(f\"Epoch {epoch}, Attack Success Rate: {attack_success_rate.item()}.\")\n",
    "\n",
    "    return patch.data, attack_success_rate.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the Pre-Trained ResNet-20 model\n",
    "net = ResNetCIFAR(num_layers=20, Nbits=None)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(\"./pretrained_models/pretrained_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Attack Success Rate: 0.38804998993873596.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Attack Success Rate: 0.6696749925613403.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Attack Success Rate: 0.6963749527931213.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Attack Success Rate: 0.7126249670982361.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    }
   ],
   "source": [
    "up, ux = patch_attack_untargeted(net, patch_size=8, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Attack Success Rate: 0.1868249922990799.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Attack Success Rate: 0.36864998936653137.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Attack Success Rate: 0.4077000021934509.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Attack Success Rate: 0.4100499749183655.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    }
   ],
   "source": [
    "# load the Pre-Trained ResNet-20 model\n",
    "net = ResNetCIFAR(num_layers=20, Nbits=None)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(\"./pretrained_models/pretrained_model.pt\"))\n",
    "up, ux = patch_attack_untargeted(net, patch_size=5, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb810273310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the patch\n",
    "p_ = (torch.tanh(up) + 1) / 2 # Parameter to pixel values\n",
    "p_ = p_.cpu().permute(1, 2, 0).numpy()\n",
    "p_ = np.clip(p_, a_min=0.0, a_max=1.0)\n",
    "plt.imshow(p_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
