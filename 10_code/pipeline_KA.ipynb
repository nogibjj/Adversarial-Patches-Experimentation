{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/workspaces/Adversarial-Patches-Experimentation/10_code/utils.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from scipy.ndimage import rotate\n",
    "from pretrained_models.resnet20 import ResNetCIFAR\n",
    "\n",
    "from utils import *\n",
    "from importlib import reload\n",
    "reload(sys.modules['utils'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outstanding Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What transformations should we apply to CIFAR-10 clean data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain CIFAR-10 Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "patch_size = 0.1\n",
    "target = 1 #automobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should we apply the below transformations?\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "\n",
    "])\n",
    "\n",
    "transform_image = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing data..')\n",
    "trainset = torchvision.datasets.CIFAR10(root='../00_data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../00_data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the Pre-Trained ResNet-20 model\n",
    "net = ResNetCIFAR(num_layers=20, Nbits=None)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(\"./pretrained_models/pretrained_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: what does mean and std mean in this context?\n",
    "1. Why are we only taking one target probability in the attack function in utils.py?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_in, max_in = 32, 32\n",
    "# min_in, max_in = np.array([min_in, min_in, min_in]), np.array([max_in, max_in, max_in])\n",
    "# #mean, std = 2, np.array(4) \n",
    "# #min_out, max_out = np.min((min_in-mean)/std), np.max((max_in-mean)/std)\n",
    "# min_out, max_out = -1, 1\n",
    "\n",
    "min_in, max_in = 32, 32\n",
    "min_in, max_in = np.array([min_in, min_in, min_in]), np.array([max_in, max_in, max_in])\n",
    "#mean, std = 2, np.array(4) \n",
    "#min_out, max_out = np.min((min_in-mean)/std), np.max((max_in-mean)/std)\n",
    "min_out, max_out = 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Shape of patch:  (256, 3, 10, 10)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of patch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, patch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m patch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcircle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m test(\n\u001b[1;32m     19\u001b[0m     epoch,\n\u001b[1;32m     20\u001b[0m     patch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     patch_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcircle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/Adversarial-Patches-Experimentation/10_code/utils.py:464\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, patch, patch_shape, netClassifier, train_loader, target, device, patch_type, image_size)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Depending on the patch type specified (patch_type), transforms the patch and mask accordingly using either a circular or square transformation.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m patch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcircle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 464\u001b[0m     patch, mask, patch_shape \u001b[38;5;241m=\u001b[39m \u001b[43mcircle_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m patch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    468\u001b[0m     patch, mask \u001b[38;5;241m=\u001b[39m square_transform(patch, data_shape, patch_shape, image_size)\n",
      "File \u001b[0;32m/workspaces/Adversarial-Patches-Experimentation/10_code/utils.py:252\u001b[0m, in \u001b[0;36mcircle_transform\u001b[0;34m(patch, data_shape, patch_shape, image_size, device)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Rotates the patch randomly using the previously generated rot angle for the specified channel (patch[0][i]) within the patch.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m##? What does the patch look like - try print(patch[0][i].shape[0])\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(patch[\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# patch[0][i] = rotate(patch[0][i], angle=rot, reshape=False)\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     patch[i][j] \u001b[38;5;241m=\u001b[39m rotate(\u001b[43mpatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m, angle\u001b[38;5;241m=\u001b[39mrot, reshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# random location\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Generates random x and y coordinates within the image size, ensuring that the patch, when placed at these coordinates,\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m#  fits within the dummy image x without exceeding its boundaries.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m random_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(image_size)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "patch, patch_shape = init_patch_circle(32, patch_size, batch_size)\n",
    "target = 8 #automobile\n",
    "for epoch in range(1, 10):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(\"Shape of patch: \", patch.shape)\n",
    "    patch = train(\n",
    "        epoch,\n",
    "        patch,\n",
    "        patch_shape,\n",
    "        net,\n",
    "        trainloader,\n",
    "        target = 8 ,\n",
    "        device=device,\n",
    "        patch_type=\"circle\",\n",
    "        image_size=32\n",
    "    )\n",
    "\n",
    "    test(\n",
    "        epoch,\n",
    "        patch,\n",
    "        patch_shape,\n",
    "        net,\n",
    "        testloader,\n",
    "        target = 8,\n",
    "        image_size = 32,\n",
    "        min_out = 0,\n",
    "        max_out = 1,\n",
    "        device=\"cuda\",\n",
    "        patch_type=\"circle\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc8d08359c0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show patch after training\n",
    "patch_image = patch[0].reshape(10, 10, 3)\n",
    "plt.imshow(patch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_application_operator(img_index, patch_path = '../00_data/patches/toaster_patch.png', h = 'center', v = 'center', ratio = 3, rotation = 0):\n",
    "    \"\"\"A Patch Application Operator function to apply a patch along with its transformations\n",
    "      to a CIFAR-10 image given an index in the training / test set.\n",
    "    \n",
    "    Args:\n",
    "        img_index (int): the index of the image in the training / test set\n",
    "        patch_path (str): the path to the patch image\n",
    "        h (str): the horizontal position of the patch (default: 'center') - see mode_to_value keys for possible values\n",
    "        v (str): the vertical position of the patch (default: 'center') - see mode_to_value keys for possible values\n",
    "        ratio (int): the ratio of the patch size to the image size (default: 3). Essentially controls the scale of the image.\n",
    "        rotation (int): the angle of rotation of the patch (default: 0)\n",
    "\n",
    "    Returns:\n",
    "        img (PIL): the image with the patch applied\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: add more transformation options for the patch\n",
    "\n",
    "    image = transform_image(trainset[img_index][0])\n",
    "    \n",
    "    patch_image = Image.open(patch_path)\n",
    "    patch_image = patch_image.resize((image.width // ratio, image.height // ratio), Image.Resampling.LANCZOS)\n",
    "    patch_image = patch_image.rotate(rotation)\n",
    "\n",
    "    # map from mode value to a function that returns the correct value position\n",
    "    mode_to_value = {\n",
    "        'left': lambda width, path: 0,\n",
    "        'center_h': lambda width, path: width // 2 - path.width // 2,\n",
    "        'center_v': lambda height, path: height // 2 - path.height // 2, \n",
    "        'right': lambda width, path: width - path.width,\n",
    "        'up': lambda height, path: 0,\n",
    "        'down': lambda height, path: height - path.height,\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(2.5, 2.5))\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    x = mode_to_value[h if h != 'center' else 'center_h'](image.width, patch_image)\n",
    "    y = mode_to_value[v if v != 'center' else 'center_v'](image.height, patch_image)\n",
    "\n",
    "    print(f\"Patch will be placed {h}-{v}\")\n",
    "\n",
    "    image.paste(patch_image, (x, y), mask = patch_image)\n",
    "    \n",
    "    plt.figure(figsize=(2.5, 2.5))\n",
    "    plt.title(\"Patched\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # convert image to tensor\n",
    "    image = transform_test(image)\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch will be placed left-up\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.9961, 0.9843, 0.9647],\n",
       "         [0.9961, 0.9961, 0.9961,  ..., 0.9882, 0.9765, 0.9569],\n",
       "         [0.9882, 0.9922, 0.8353,  ..., 0.9804, 0.9686, 0.9529],\n",
       "         ...,\n",
       "         [0.6863, 0.6784, 0.6784,  ..., 0.5882, 0.5765, 0.5529],\n",
       "         [0.6863, 0.6784, 0.6745,  ..., 0.5647, 0.5373, 0.5294],\n",
       "         [0.6745, 0.6784, 0.6706,  ..., 0.5765, 0.5412, 0.5020]],\n",
       "\n",
       "        [[1.0000, 1.0000, 1.0000,  ..., 0.9569, 0.9451, 0.9373],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.9490, 0.9373, 0.9294],\n",
       "         [0.9922, 1.0000, 0.4392,  ..., 0.9412, 0.9294, 0.9255],\n",
       "         ...,\n",
       "         [0.6902, 0.6824, 0.6824,  ..., 0.6039, 0.5961, 0.5922],\n",
       "         [0.6902, 0.6824, 0.6784,  ..., 0.5804, 0.5569, 0.5686],\n",
       "         [0.6784, 0.6824, 0.6745,  ..., 0.5922, 0.5608, 0.5373]],\n",
       "\n",
       "        [[1.0000, 1.0000, 1.0000,  ..., 0.9529, 0.9412, 0.9333],\n",
       "         [0.9843, 0.9843, 0.9843,  ..., 0.9451, 0.9333, 0.9216],\n",
       "         [0.9686, 0.9765, 0.6196,  ..., 0.9373, 0.9255, 0.9059],\n",
       "         ...,\n",
       "         [0.6706, 0.6627, 0.6627,  ..., 0.6078, 0.6000, 0.6000],\n",
       "         [0.6706, 0.6627, 0.6588,  ..., 0.5843, 0.5608, 0.5765],\n",
       "         [0.6588, 0.6627, 0.6549,  ..., 0.6000, 0.5647, 0.5451]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_application_operator(240, h = 'left', v = 'up', ratio = 2, rotation = 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models on Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epochs, batch_size, lr, reg, \n",
    "          transform_train=transform_train, \n",
    "          transform_test=transform_test, \n",
    "          log_every_n=50, \n",
    "          device = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Training a network\n",
    "    :param net: Network for training\n",
    "    :param epochs: Number of epochs in total.\n",
    "    :param batch_size: Batch size for training.\n",
    "    \"\"\"\n",
    "    print('==> Preparing data..')\n",
    "\n",
    "    best_acc = 0  # best test accuracy\n",
    "    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "    trainset = torchvision.datasets.CIFAR10(root='../00_data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='../00_data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.875, weight_decay=reg, nesterov=False)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs*0.5), int(epochs*0.75)], gamma=0.1)\n",
    "\n",
    "    global_steps = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        \"\"\"\n",
    "        Start the training code.\n",
    "        \"\"\"\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            global_steps += 1\n",
    "\n",
    "            if global_steps % log_every_n == 0:\n",
    "                end = time.time()\n",
    "                num_examples_per_second = log_every_n * batch_size / (end - start)\n",
    "                print(\"[Step=%d]\\tLoss=%.4f\\tacc=%.4f\\t%.1f examples/second\"\n",
    "                      % (global_steps, train_loss / (batch_idx + 1), (correct / total), num_examples_per_second))\n",
    "                start = time.time()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        \"\"\"\n",
    "        Start the testing code.\n",
    "        \"\"\"\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        num_val_steps = len(testloader)\n",
    "        val_acc = correct / total\n",
    "        print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Saving...\")\n",
    "            torch.save(net.state_dict(), \"pretrained_model.pt\")\n",
    "\n",
    "def test(net, device = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "\n",
    "    ])\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='../00_data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test accuracy=%.4f\" % (test_loss / (num_val_steps), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/codespace/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/codespace/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100.0%\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet161_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet161_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /home/codespace/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# ResNet, DenseNet and VGG\n",
    "\n",
    "# Load the pretrained model from pytorch - okay this is probably wrong. look into this\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "vgg = torchvision.models.vgg16(pretrained=True)\n",
    "densenet = torchvision.models.densenet161(pretrained=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/multiprocessing/queues.py\", line 244, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 428, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/storage.py\", line 297, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/storage.py\", line 330, in _share_fd_cpu_\n",
      "    return super()._share_fd_cpu_(*args, **kwargs)\n",
      "RuntimeError: unable to write to file </torch_12044_1703381552_1>: No space left on device (28)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 12050) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(resnet, \u001b[39m20\u001b[39;49m, batch_size, lr\u001b[39m=\u001b[39;49m\u001b[39m0.002\u001b[39;49m, reg\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[21], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, epochs, batch_size, lr, reg, transform_train, transform_test, log_every_n, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader):\n\u001b[0;32m---> 40\u001b[0m     inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39;49mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m     outputs \u001b[39m=\u001b[39m net(inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(previous_handler)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 12050) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit."
     ]
    }
   ],
   "source": [
    "train(resnet, 20, batch_size, lr=0.002, reg=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST CELLS - IGNORE STUFF BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path will be placed right-center\n",
      "Image saved at: ../00_data/test_images/banana-patched.jpeg\n"
     ]
    }
   ],
   "source": [
    "h = 'right'\n",
    "v = 'center'\n",
    "ratio = 3\n",
    "\n",
    "PATCH_PATH = '../00_data/patches/toaster_patch.png'\n",
    "src_img = '../00_data/test_images/banana.jpeg'\n",
    "save_path = '../00_data/test_images/banana-patched.jpeg'\n",
    "\n",
    "mode = {\n",
    "    'h': h,\n",
    "    'v': v\n",
    "}\n",
    "\n",
    "print(f\"Path will be placed {mode['h']}-{mode['v']}\")\n",
    "\n",
    "# map from mode value to a function that returns the correct value position\n",
    "mode_to_value = {\n",
    "    'left': lambda width, path: 0,\n",
    "    'center_h': lambda width, path: width // 2 - path.width // 2,\n",
    "    'center_v': lambda height, path: height // 2 - path.height // 2,\n",
    "    'right': lambda width, path: width - path.width,\n",
    "    'up': lambda height, path: 0,\n",
    "    'down': lambda height, path: height - path.height,\n",
    "}\n",
    "\n",
    "def get_position(mode, image, path):\n",
    "    x = mode_to_value[mode['h'] if mode['h'] != 'center' else 'center_h'](image.width, path)\n",
    "    y = mode_to_value[mode['v'] if mode['v'] != 'center' else 'center_v'](image.height, path)\n",
    "    return x, y\n",
    "\n",
    "patch_image = Image.open(PATCH_PATH)\n",
    "src_image = Image.open(src_img)\n",
    "\n",
    "patch_image = patch_image.resize((src_image.width // ratio, src_image.height // ratio), Image.Resampling.LANCZOS)\n",
    "position = get_position(mode, src_image, patch_image)\n",
    "\n",
    "src_image.paste(patch_image, position, mask = patch_image)\n",
    "src_image.save(save_path)\n",
    "\n",
    "print(f\"Image saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
