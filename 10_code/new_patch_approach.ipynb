{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import pacakges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 19:27:21.047400: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-14 19:27:21.047505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-14 19:27:21.455355: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-14 19:27:22.253686: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-14 19:27:26.490776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/workspaces/Adversarial-Patches-Experimentation/10_code/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from scipy.ndimage import rotate\n",
    "from pretrained_models.resnet20 import ResNetCIFAR\n",
    "\n",
    "from utils import *\n",
    "from importlib import reload\n",
    "reload(sys.modules['utils'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# PARAMS\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "patch_size = 0.1\n",
    "target = 1 #automobile\n",
    "\n",
    "#CIFAR-10 image tensor mean and std\n",
    "NORM_MEAN = [0.4914, 0.4822, 0.4465]\n",
    "NORM_STD = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: should we apply the below transformations?\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN,\n",
    "                         std=NORM_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN,\n",
    "                         std=NORM_STD)\n",
    "])\n",
    "\n",
    "transform_image = transforms.ToPILImage()\n",
    "\n",
    "print('==> Preparing data..')\n",
    "trainset = torchvision.datasets.CIFAR10(root='../00_data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../00_data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR_MEANS, TENSOR_STD = torch.FloatTensor(NORM_MEAN)[:,None,None], torch.FloatTensor(NORM_STD)[:,None,None]\n",
    "def patch_forward(patch):\n",
    "    # Map patch values from [-infty,infty] to ImageNet min and max\n",
    "    patch = (torch.tanh(patch) + 1 - 2 * TENSOR_MEANS) / (2 * TENSOR_STD)\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_patch(img, patch):\n",
    "    for i in range(img.shape[0]):\n",
    "        h_offset = np.random.randint(0,img.shape[2]-patch.shape[1]-1)\n",
    "        w_offset = np.random.randint(0,img.shape[3]-patch.shape[2]-1)\n",
    "        img[i,:,h_offset:h_offset+patch.shape[1],w_offset:w_offset+patch.shape[2]] = patch_forward(patch)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_patch(model, patch, val_loader, target_class):\n",
    "    model.eval()\n",
    "    tp, tp_5, counter = 0., 0., 0.\n",
    "    n = 0 # number of images\n",
    "    with torch.no_grad():\n",
    "        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n",
    "            # For stability, place the patch at 4 random locations per image, and average the performance\n",
    "            for _ in range(4):\n",
    "                patch_img = place_patch(img, patch)\n",
    "                patch_img = patch_img.to(device)\n",
    "                img_labels = img_labels.to(device)\n",
    "                pred = model(patch_img)\n",
    "                # In the accuracy calculation, we need to exclude the images that are of our target class\n",
    "                # as we would not \"fool\" the model into predicting those\n",
    "                tp += torch.logical_and(pred.argmax(dim=-1) == target_class, img_labels != target_class).sum()\n",
    "                tp_5 += torch.logical_and((pred.topk(5, dim=-1)[1] == target_class).any(dim=-1), img_labels != target_class).sum()\n",
    "                counter += (img_labels != target_class).sum()\n",
    "                n += (img_labels != target_class).sum()\n",
    "    acc = tp/counter\n",
    "    top5 = tp_5/counter\n",
    "    attack_success_rate = tp/n\n",
    "    return acc, top5, attack_success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_attack(model, target_class, patch_size=10, num_epochs=5):\n",
    "    # Leave a small set of images out to check generalization\n",
    "    # In most of our experiments, the performance on the hold-out data points\n",
    "    # was as good as on the training set. Overfitting was little possible due\n",
    "    # to the small size of the patches.\n",
    "    train_set, val_set = torch.utils.data.random_split(trainset, [0.8, 0.2])\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last = True, num_workers=4)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "    # Create parameter and optimizer\n",
    "    if not isinstance(patch_size, tuple):\n",
    "        patch_size = (patch_size, patch_size)\n",
    "    patch = nn.Parameter(torch.zeros(3, patch_size[0], patch_size[1]), requires_grad=True)\n",
    "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        t = tqdm(train_loader, leave=False)\n",
    "        for img, _ in t:\n",
    "            img = place_patch(img, patch)\n",
    "            img = img.to(device)\n",
    "            pred = model(img)\n",
    "            labels = torch.zeros(img.shape[0], device=pred.device, dtype=torch.long).fill_(target_class)\n",
    "            loss = loss_module(pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n",
    "        if epoch % 5 == 0:\n",
    "            acc, top5, atttack_success_rate = eval_patch(model, patch, val_loader, target_class)\n",
    "            print(f\"Epoch {epoch}, Attack Success Rate: {atttack_success_rate.item():4.2f}\")\n",
    "\n",
    "    # Final validation\n",
    "    # acc, top5 = eval_patch(model, patch, val_loader, target_class)\n",
    "\n",
    "    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item(), \"attack_success_rate\": atttack_success_rate.item()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the Pre-Trained ResNet-20 model\n",
    "net = ResNetCIFAR(num_layers=20, Nbits=None)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(\"./pretrained_models/pretrained_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Attack Success Rate: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Attack Success Rate: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Attack Success Rate: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Attack Success Rate: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Attack Success Rate: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Attack Success Rate: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    }
   ],
   "source": [
    "p, x = patch_attack(net, target_class=8, patch_size=16, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0598, device='cuda:0'), tensor(0.4388, device='cuda:0'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check test accuracy\n",
    "eval_patch(net, p, testloader, target_class=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show patch\n",
    "# plt.imshow(transform_image(patch_forward(p).cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ = (torch.tanh(p) + 1) / 2 # Parameter to pixel values\n",
    "p_ = p_.cpu().permute(1, 2, 0).numpy()\n",
    "p_ = np.clip(p_, a_min=0.0, a_max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f639669be20>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(p_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
