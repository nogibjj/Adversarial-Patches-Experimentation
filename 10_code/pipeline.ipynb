{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 07:22:16.487490: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-11 07:22:16.487557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-11 07:22:16.488827: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-11 07:22:16.498769: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-11 07:22:17.759078: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/workspaces/Adversarial-Patches-Experimentation/10_code/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from scipy.ndimage import rotate\n",
    "from pretrained_models.resnet20 import ResNetCIFAR\n",
    "\n",
    "from utils import *\n",
    "from importlib import reload\n",
    "reload(sys.modules['utils'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outstanding Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What transformations should we apply to CIFAR-10 clean data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain CIFAR-10 Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "patch_size = 0.1\n",
    "target = 1 #automobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should we apply the below transformations?\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "\n",
    "])\n",
    "\n",
    "transform_image = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing data..')\n",
    "trainset = torchvision.datasets.CIFAR10(root='../00_data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../00_data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the Pre-Trained ResNet-20 model\n",
    "net = ResNetCIFAR(num_layers=20, Nbits=None)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(\"./pretrained_models/pretrained_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: what does mean and std mean in this context?\n",
    "1. Why are we only taking one target probability in the attack function in utils.py?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_in, max_in = 32, 32\n",
    "min_in, max_in = np.array([min_in, min_in, min_in]), np.array([max_in, max_in, max_in])\n",
    "#mean, std = 2, np.array(4) \n",
    "#min_out, max_out = np.min((min_in-mean)/std), np.max((max_in-mean)/std)\n",
    "min_out, max_out = 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Adversarial-Patches-Experimentation/10_code/utils.py:310: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x_out = F.softmax(netClassifier(x))\n",
      "/workspaces/Adversarial-Patches-Experimentation/10_code/utils.py:327: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  adv_out = F.log_softmax(netClassifier(adv_x))\n",
      "/workspaces/Adversarial-Patches-Experimentation/10_code/utils.py:344: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.softmax(netClassifier(adv_x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Patch Success: 0.691\n",
      "Epoch 1: Test Patch Success: 0.109\n",
      "Epoch:  2\n"
     ]
    }
   ],
   "source": [
    "patch, patch_shape = init_patch_circle(32, patch_size, batch_size)\n",
    "target = 8 #automobile\n",
    "for epoch in range(1, 10):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(\"Shape of patch: \", patch.shape)\n",
    "    patch = train(\n",
    "        epoch,\n",
    "        patch,\n",
    "        patch_shape,\n",
    "        net,\n",
    "        trainloader,\n",
    "        target = 8 ,\n",
    "        device=device,\n",
    "        patch_type=\"circle\",\n",
    "        image_size=32\n",
    "    )\n",
    "\n",
    "    test(\n",
    "        epoch,\n",
    "        patch,\n",
    "        patch_shape,\n",
    "        net,\n",
    "        testloader,\n",
    "        target = 8,\n",
    "        image_size = 32,\n",
    "        min_out = 0,\n",
    "        max_out = 1,\n",
    "        device=\"cuda\",\n",
    "        patch_type=\"circle\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_application_operator(img_index, patch_path = '../00_data/patches/toaster_patch.png', h = 'center', v = 'center', ratio = 3, rotation = 0):\n",
    "    \"\"\"A Patch Application Operator function to apply a patch along with its transformations\n",
    "      to a CIFAR-10 image given an index in the training / test set.\n",
    "    \n",
    "    Args:\n",
    "        img_index (int): the index of the image in the training / test set\n",
    "        patch_path (str): the path to the patch image\n",
    "        h (str): the horizontal position of the patch (default: 'center') - see mode_to_value keys for possible values\n",
    "        v (str): the vertical position of the patch (default: 'center') - see mode_to_value keys for possible values\n",
    "        ratio (int): the ratio of the patch size to the image size (default: 3). Essentially controls the scale of the image.\n",
    "        rotation (int): the angle of rotation of the patch (default: 0)\n",
    "\n",
    "    Returns:\n",
    "        img (PIL): the image with the patch applied\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: add more transformation options for the patch\n",
    "\n",
    "    image = transform_image(trainset[img_index][0])\n",
    "    \n",
    "    patch_image = Image.open(patch_path)\n",
    "    patch_image = patch_image.resize((image.width // ratio, image.height // ratio), Image.Resampling.LANCZOS)\n",
    "    patch_image = patch_image.rotate(rotation)\n",
    "\n",
    "    # map from mode value to a function that returns the correct value position\n",
    "    mode_to_value = {\n",
    "        'left': lambda width, path: 0,\n",
    "        'center_h': lambda width, path: width // 2 - path.width // 2,\n",
    "        'center_v': lambda height, path: height // 2 - path.height // 2, \n",
    "        'right': lambda width, path: width - path.width,\n",
    "        'up': lambda height, path: 0,\n",
    "        'down': lambda height, path: height - path.height,\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(2.5, 2.5))\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    x = mode_to_value[h if h != 'center' else 'center_h'](image.width, patch_image)\n",
    "    y = mode_to_value[v if v != 'center' else 'center_v'](image.height, patch_image)\n",
    "\n",
    "    print(f\"Patch will be placed {h}-{v}\")\n",
    "\n",
    "    image.paste(patch_image, (x, y), mask = patch_image)\n",
    "    \n",
    "    plt.figure(figsize=(2.5, 2.5))\n",
    "    plt.title(\"Patched\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # convert image to tensor\n",
    "    image = transform_test(image)\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch will be placed left-up\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANQAAADqCAYAAADarmvFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaD0lEQVR4nO2deZBV5ZnG37vf2+vtnV7pplkaaAMCQlTAhCGFhkhhqVOiJRBjqMpE/zBVU5Wq+YOqjIWVqRSTqkSZmZSFhhqDioIV1LAo40gWFQGVfZFmaei9b2/39t3ON384dmh5XjxNPjXq86vyD58+fc53z71PH/rp93tfjzHGCCHECt4vegGEfJWgoQixCA1FiEVoKEIsQkMRYhEaihCL0FCEWISGIsQiNBQhFqGhvoQ89dRT4vF4ZN++fZ/5tVavXi319fWf+XW+KtBQY+DjD/LH/4XDYZk8ebI89NBD0t7ePqZzrVu3TrZt2/bZLJR8Yfi/6AV8GfnZz34mDQ0NMjw8LHv37pUNGzbIK6+8IocOHZKcnBxX51i3bp3cddddsnz58s92seRzhYa6Bm677TaZM2eOiIg8+OCDUlJSIuvXr5eXXnpJVqxY8QWvjnyR8J98Fli0aJGIiJw5c0Z+8YtfyE033SQlJSUSiURk9uzZsmXLllHHezweGRoakqeffnrkn4+rV68e+Xpra6v84Ac/kKqqKgmFQtLQ0CA/+tGPJJVKjTpPMpmUn/zkJ1JWVia5ublyxx13SGdn5xXre/XVV2XBggWSm5sr+fn5snTpUjl8+PAVx23btk2am5slHA5Lc3OzbN261cLd+XrBJ5QFTp8+LSIiJSUl8uijj8qyZcvkvvvuk1QqJZs3b5a7775btm/fLkuXLhURkU2bNsmDDz4oc+fOlTVr1oiISGNjo4iIXLx4UebOnSuxWEzWrFkjTU1N0traKlu2bJF4PC7BYHDkug8//LAUFRXJ2rVrpaWlRX75y1/KQw89JM8+++zIMZs2bZJVq1bJkiVL5Oc//7nE43HZsGGDzJ8/Xw4cODASOOzcuVPuvPNOmTZtmjz22GPS3d0t3//+96WmpubzuIVfHQxxzcaNG42ImN27d5vOzk5z/vx5s3nzZlNSUmIikYi5cOGCicfjo74nlUqZ5uZms2jRolF6bm6uWbVq1RXXWLlypfF6veadd9654muO44xax+LFi0c0Y4x55JFHjM/nM7FYzBhjzMDAgIlGo+aHP/zhqPO0tbWZwsLCUfrMmTNNZWXlyPcaY8zOnTuNiJjx48e7u0HE8J9818DixYulrKxMamtr5Z577pG8vDzZunWrVFdXSyQSGTmut7dX+vr6ZMGCBbJ///5PPa/jOLJt2za5/fbbR35HuxyPxzPq/9esWTNKW7BggWSzWTl79qyIiOzatUtisZisWLFCurq6Rv7z+Xwyb9482bNnj4iIXLp0SQ4ePCirVq2SwsLCkfN95zvfkWnTpo3t5nzN4T/5roHHH39cJk+eLH6/XyoqKmTKlCni9X70s2n79u3y6KOPysGDByWZTI58zyfNgOjs7JT+/n5pbm52tY66urpR/19UVCQiHxlZROTkyZMi8tff8T5JQUGBiMiIASdNmnTFMVOmTHH1w4B8BA11DcydOxc+Qd58801ZtmyZLFy4UJ544gmprKyUQCAgGzdulGeeecb6Onw+H9TN/3c1cBxHRD76PWrcuHFXHOf38+23De+oRV544QUJh8OyY8cOCYVCI/rGjRuvOBY9scrKyqSgoEAOHTpkZT0fBx3l5eWyePFi9bjx48eLyF+faJdz/PhxK2v5usDfoSzi8/nE4/FINpsd0VpaWmBFRG5ursRisVGa1+uV5cuXy+9//3tYVmTG2E9nyZIlUlBQIOvWrZN0On3F1z+O2CsrK2XmzJny9NNPS19f38jXd+3aJUeOHBnTNb/u8AllkaVLl8r69evl1ltvlXvvvVc6Ojrk8ccfl4kTJ8r7778/6tjZs2fL7t27Zf369VJVVSUNDQ0yb948WbdunezcuVNuueUWWbNmjUydOlUuXbokzz//vOzdu1ei0ajr9RQUFMiGDRvk/vvvl1mzZsk999wjZWVlcu7cOXn55Zfl5ptvll//+tciIvLYY4/J0qVLZf78+fLAAw9IT0+P/OpXv5Lp06fL4OCgzdv01eaLjhm/THwcV6NI+2OefPJJM2nSJBMKhUxTU5PZuHGjWbt2rfnkrT527JhZuHChiUQiRkRGRehnz541K1euNGVlZSYUCpkJEyaYH//4xyaZTF51HXv27DEiYvbs2XOFvmTJElNYWGjC4bBpbGw0q1evNvv27Rt13AsvvGCmTp1qQqGQmTZtmnnxxRfNqlWrGJuPAY8x7MtHiC34OxQhFqGhCLEIDUWIRWgoQixCQxFiERqKEIvQUIRY5HOvlDDGgfqFI2eh/vaeP0P9TMsFqA8qzVKq0vjPbVMrKqFeVDkR6tHbr4d6zfR6qH+ZGPufJLXj8XssgivutTp87bNinLHp3nQW6peXiF1OWjlPJFoE9VHX+tQjCCGuoaEIsQgNRYhFaChCLOI6lDCSsXLBlzZth/reV9+AeiA/DPVJTdOhXjdpCtRf+82Vm/xEROr7g1BvGvo21N/4Fm6t5f+Xeqgve3g51L2+T98S//eOtqvfGO0Lyom0MEQJB8TBYYJ2vDNmXTm/C/iEIsQiNBQhFqGhCLEIDUWIRWgoQiziOuVzHJzyebzYk8ce+W+onz7wPtT7p+MxMFXhQqjnlNdBvav1HNSvm/oNqOdFcGfU/IIBqIezcagfe7kf6rsmnoD6ku/hNFILvFz0ybSAWgSEVWWtagmTg3WPWsGkfCFrpyRJS/mMufZEm08oQixCQxFiERqKEIvQUIRYhIYixCKuG106ZhjqOzc8B/XNv9kN9VReDOqVs2ZAvai0AeqlxXiyXue5Kxvei4gUGJwiBvy4lm/Oa6eg7njxBsNL91ZDPZF/5dQLEZHCnG6oL/3H66B+dZQUbmyHX8N5lFo7Pf7DurYfUampM5kr+7SL6OvMgL7uIiJZZf1aLV9+Kd6Mejl8QhFiERqKEIvQUIRYhIYixCI0FCEWcV3Ld/RQC9R3n8bDuLqux+lWciAF9WgPbtGUznRAvSdxHup1lQugnh1W6sguGyx9OWce+CeoD/QqIzJ78BjP3rajUN/xPm6DNm1mPtQbJuFU82o4ag0e1r1aLZ+a2uE0zKPmbfjnd1bbmau0+fJo6Z92HiVGzCj1qSZ17ROe+IQixCI0FCEWoaEIsQgNRYhFaChCLOI65Tv4/ntQL67HaV5jCKdnJ7p6od5eg2vnAoeKoT75PpyGtbf5oD6rDqeI4Rq88/fUAZzCzarBfQILqudA/fk//AkfX4aTqoNH9kG9fkIV1K+GlrZp+3K1n65aKqjpmSxOz7wOvrJP0TWUcFE8Whu/jJLmKSmiSWtp4afDJxQhFqGhCLEIDUWIRWgoQixCQxFiEdcp36H33sInKMI7TAtLK6DuBHB6NpxzDF94Rh6Uh7pw+jc0pRPquWG8s3jD2laoP3xvCdTfOx6Beon/dqgX1UWhPtB7BupHO/4I9eXZpVC/KkpfOy2d0wd5aimckpIp6VlyGO+cjQRwcupoKaLyurIZ/AqyDn5uOEpDwGwAyq7gE4oQi9BQhFiEhiLEIjQUIRahoQixiOuUb6AHp2HBYBPUi0txjV+gGKdn8fbJUG9cgJeY9uP15ATw1IxXD+Idvm+8hdPLGTPxjN1UHE/ZiOfhWsTcYjw9ZCiN+xwmO2NQN9mr7CLVQjgl9XKUPnVa+hfw4/fAUVK+bBafP61c16P8XPcp140rtYKBII7nhodxXWnPIP6s/PG9d6D+wF33Q/1y+IQixCI0FCEWoaEIsQgNRYhFaChCLOI65TMlOEpyEjgp0XqtjauZAPUD7+EUrrcf78B1wh9CPTeIX1I4H1+3uQnXEHb04n6DVUV4iseFc0egPjEfz/aNJWL4+DjeWexkcD9DERGPsuM1I/h7EsoIWb+SCnqU2jnHr9TapfEFMkpSaQL4+FPncN1nIIDf4+On8eSV2ooyqD/xu6egfqIVf7aY8hHyOUNDEWIRGooQi9BQhFiEhiLEIq5TvoYQniF7QWmSlk7ihGlC4ySov/YHvKO2rycK9WR5DOqVBfVQDwbweSZOxa9rwezxUE8n8C373x0HoF5dj2cESwCnl/nj8Mxfj0fvXZfxKv3uDK5t8yqJYVa5RFZJ1XriOOFNxBNQHxzC+v6j+N5t2voS1GsqyqF+5hzeBT19UiO+7okPoB6JXPuWXT6hCLEIDUWIRWgoQixCQxFiERqKEIu4TvnGz8X98XxH8I7U1/efhvqtd86GeiAf9/ELpgqgPqv0JqhnO3ANXiYRgnpNTSnU39j/JtS/dcOtUP/pw/dCvbB6CtQ/bMUJWVU1vj9vK9NPRER2/vnPUL9l7nyo+zN45+zRc7iGLa3UZe77AKdkPg/+Oe1VwrMT5/F12/vxpJaU4B244sXrfPcYnn9svDhpzSaU87uATyhCLEJDEWIRGooQi9BQhFiEhiLEIh6jNWP7BMfb90D9uSdfgfoTv8FTMP7juf+E+lASJzon3t4J9cSw0tcughOsOqX+q/sSnqbR9uZPoT7z5llQv6EG9yHsyeKfWQdO4xrIaO2NUH997xaoi4gcu4B3to6L4oRU2/Hal8T3bmgwjnWl313DOJzY5uXhpDWRwR/B7oFuqAeUXdlFSrp4pqMd6t4I3n1dHsXJ9Z5N26E+6pyfegQhxDU0FCEWoaEIsQgNRYhFaChCLOK6lq/lPVwjN76uFuq3LcY7VesyuCatKRf35ftDAk/H+DCJa/CaZ38T6tHed6G+L4jrv6bccgvU9x/AffzefesE1L/3LVxPVzO5Dur/8y5ONdNh/a2qb6iHeu5wDOrdcZza9aZw8uj4cM1bYRnuIRiOKNM0PPj8FYW4b55XcOqYMljPU1K+nDBO83Lyo1APhPOh7gY+oQixCA1FiEVoKEIsQkMRYhEaihCLuE75Nm3FdUzfmD4d6tdfNwPqE88+C/X/+td/h/riFd+F+vIFYaj/9pl/xnoWr8fnwfVlQx5cm1dcdhvUq4uLoX44g5OwOoN37N5wXTPUYzFc6ygicuok3vHaF8NpmGg9/nAIJ+W5uDavBI9Llqo8nJKllI9bbTm+18EQXn9Qme7R0YPrR4uKcG1eTgDXcYZcu+JK+IQixCI0FCEWoaEIsQgNRYhFaChCLOI6zzjaimvtznW0Qd3n/RPU85rroT6zGfevS54+BfUdh89B/XfHKqEeC+Ak7JszcK3d1HqcADlJvPuzcwDfn55OvLO46yxuUnfywlmoX0zhWkoRkYwyASWi7IQdUPrshYJ4h6+227l/oAXquaW4znJcMX5vCvz4XvgqcLqozcaIKzV+A70x/A0ZnHb2x/F77AY+oQixCA1FiEVoKEIsQkMRYhEaihCLuE75PBFl9qsyUmFoeAjq/7ZvP9RvLMP97uYkL0LddPRA/ZG7J0A9HopC3efgdO7tBE7nDrQr82OVPoHXV9VAvb4KJ1hdcdzrrvUi7r0nIpJO4R24wRB+b/xe/LZHlZ2qU+pwOtdyEdcXVpThIr+AsnM2mFbSuRS+FwmlFHFQ2YnscXAKOpzC1zUOfi/dwCcUIRahoQixCA1FiEVoKEIsQkMRYhHXKZ8yvlR8PnwK4+AoJiw4LezN4t5vTw7gmrpEHu5rF/wLTgUjwQ6oZ5IpqHt8OKVMKsnT5BqchE2sxD3nskoyl+PB96EigHedioi0dnVBvawWr2lQmbIxLi8K9Ruua4J61sHTMU6dOA71nEJcE1iamwv185dwvWb3EH5vcoI4pezow/fHF8CfUZ/jaiANhE8oQixCQxFiERqKEIvQUIRYhIYixCI0FCEWcR2bl4ZwY8lwCBc8FiujQkIBHJuvvOMuqL/8xg6op5URK91teEv+4EAM6pEi3IgyqRROFkbwdb99PW6kWVuGt4N3K40rF944D+oxo/zdQkSOHMGNIgv8+Ofl0CD+U0F1Nf5TRCaBt99rH57hJP6TwMlDuDD65tlzoJ528Dr7+vF65t6MB36LF79nldW4cLmrrQWfxwV8QhFiERqKEIvQUIRYhIYixCI0FCEWcZ3yjS/DhYdKbamUl1VDvUyZgeIYvJSKAlwU6lUGFJfX4BQxEsGNNP0+fJ6wsn18Qh1OhmY04a33/f14bE3PkaNQ77yoFLrW4MJSEZGJlfieppVt/DOmX4+vUYwbXQaVItLYIE7bkkoSmvDjpLJ9AK/TeHBhdDiAj+/sxW0RhlP4+A9P4ean/QM4KXYDn1CEWISGIsQiNBQhFqGhCLEIDUWIRVynfN+cieutAgGchkWjOJUqyMWp3c4dO6He0FgF9SmTcWqnrcdkccJkHDwA2e/Benk53tKek4PTxVAwCvWs0tzx7FnceHPaeDzMWkTECeEmmGmlzrJuHB6wnUrh1/yXt96B+rHjJ6F+sQ/XKebm4rrGYAR/Vi514fNcuISHU6fTx6De1o3H0+REcHKdiPdB3Q18QhFiERqKEIvQUIRYhIYixCI0FCEWcZ3yNTdNg7pPqYVLKc0Ue7txQpMXws0Fw16sJ/pxApRVUj6/V6vZw+mc36fMTFFSwUwa38qCwijUJ05ohPrBI7i+rL0fN30UEWnvw/doOI7rAhtrcA3e0DB+bQNxXAu3fPH3oN7X3w/17952q3JdnMDuP14P9d2v4/emrnI81EuVlC8vNwr1eCIGdTfwCUWIRWgoQixCQxFiERqKEIvQUIRYxHXKFxvAyY3fg1O1bAYnRhdaW6AexJszpbsHj6Fp62iFukeZROL343WmlHX6giGo5+Xg0Ss1FXh0TENDPdRrq2uhPu0buGavYZy+Y3fSlOug/tyLv4V6PIkHb4f8uPYvX9k1XVGKd2U3jNf63eEE88hpPBanO4GT4n+4cRHUA16c2E6ZhOs+nTR+nmQyuK+gG/iEIsQiNBQhFqGhCLEIDUWIRWgoQiziOuXzePGh6UxW+QZcb1Veg5Mh5zweBp1fgHdV+gN4GohfGfrs9OL+eIE8nGwNZPB6fEqt4KWuS1B/c99bUA9HcKwZzcPTQLqUvoUiIqVKANg8Gad/5ZUNUK+rw7VwTbNnQ/3sWZy0Op4Y1Hu68E7Y/iz+rAylcI1fjZLABnw45UtlcVooSr/BrBd/ttzAJxQhFqGhCLEIDUWIRWgoQixCQxFiEdcp39DAENR9AZyqadMxfMqM3XAYJzcmi3eL+sPKlA0lAdLq11JJ/LqiyvnDYZzyJUpwjV9fIgr14QQeW1KATy89Xbj3nohI/wCudxQlgD10+BDUW9vwDt8KpU6xvQ1fNxDB97q0GPc0/LAbv7ZMWpnKAVWRjIPTvGxW2WWt6H8LfEIRYhEaihCL0FCEWISGIsQiNBQhFnGd8vkD+FC/H+vZLI6YvEqtXZ7Sv254CM9NTcZxb7lMGqdn+eV412lmEKd8mSGsDyt1YeFSPNFixgRcN2cE15EFlbQzGcPrERFJeHHN23AGr7WvD9fUDSdwyvfBBy1Qr5/QBPXuAVw3+drB16F+sh3v2PV58qDeWIaneCSV1xtR6ia9fqXu08H30w18QhFiERqKEIvQUIRYhIYixCI0FCEWcZ3yBYO4tk3Dq0y70Hby+gpxSvbW23+EejqpzMAdh+vOqpVkKKK9rhBOBf0+ZeeywRVmvb0xqIeVWsH+OO5/mKf0zBMRCYXxDlOjbFStqqzAelUJ/gal2eEHh/E84OPn26De1oknr8TjuPZvQk0d1BPK8UnB6VxsEKeOySTelf23wCcUIRahoQixCA1FiEVoKEIsQkMRYhHXKZ9Wm+dRUjst5RtSauROtxyDetLgeqtgDq7z6urF548ptXAhZWdxUqkJHFeE08juIVwf19qJ6+OKCwugXlqMz59JK9tvRSSgJI/DaZyGaXOROzrw1Ayf8h6nDU4qz53D/fqyynxi4+D3eCCmpHO5+N71aa/Xq3yGlIRX+0y7gU8oQixCQxFiERqKEIvQUIRYhIYixCLuUz4Hp0xZZXejtuuxvx8nNykc0Eht1USoh0K4fk2bsZtVpoR0dbRDPZnAdV6dg8r8VT9eT3EJrpvLKLtLL3bG8HquUnempVJDw3itwQDeweoJ4ITRyeL3srISp4Lz594I9dNnTkB9IIHTv+IS3MevohZPcImmlHvk4A9FJILrIxPK3GU38AlFiEVoKEIsQkMRYhEaihCL0FCEWMR1ypdWkg+j7FRNpXAtnHaeggK8Q1arOwuG8JiKbAYnUv4c/FKDIVzP1deHp2lo/Qm1urDBQdw/sKML7151DE4jw0oiJSJiDH7Nxodr2OrrG6FeUoRTNUdJSP3KJJWUEtkWR/GO4Jxc/BnSsraMkjgHlPnHmWH8WUwpyWlWG+/hAj6hCLEIDUWIRWgoQixCQxFiERqKEIu4Tvm02jxN9ykJU44yCcEo9VZeH65TCynpnDes/IwwYztPUEmMtHmtXuX1GuVnVpGSRmo7mrX7KSIyrKRV4TC+16kEnl3r5OPXFlTmFisbYSWgJJ6hEF6P+PG90NK2jFZrp3yGRPmMaolziikfIX8f0FCEWISGIsQiNBQhFqGhCLGI65RvrGg1ftruUi25CXjHtkSP0mdPQxunqumpNP5CYgCnc1p6GQwqiZfg6EybEyui7/7NzcHpXDik1SnidE6tkcvgGjnt3gWU8ztepVZQiRETCVwrqO0sDmi9F5V6U0fpc+gGPqEIsQgNRYhFaChCLEJDEWIRGooQi3xm0ze0lE+r/dOSJE3XrptSerOlUjhFdJTaPDWdU9aj7f5MG3z+dBonc/E47qU3NKT0AxSRSAT3BEzE8W7h2ho8uUSra3TG+F56PfhjFVJqAjNJXFuYES0hxes0yno8yuASjwe/B8lhvB438AlFiEVoKEIsQkMRYhEaihCL0FCEWMRjtDiOEDJm+IQixCI0FCEWoaEIsQgNRYhFaChCLEJDEWIRGooQi9BQhFiEhiLEIv8HHNN3G81a8+oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.9961, 0.9843, 0.9647],\n",
       "         [0.9961, 0.9961, 0.9961,  ..., 0.9882, 0.9765, 0.9569],\n",
       "         [0.9882, 0.9922, 0.8353,  ..., 0.9804, 0.9686, 0.9529],\n",
       "         ...,\n",
       "         [0.6863, 0.6784, 0.6784,  ..., 0.5882, 0.5765, 0.5529],\n",
       "         [0.6863, 0.6784, 0.6745,  ..., 0.5647, 0.5373, 0.5294],\n",
       "         [0.6745, 0.6784, 0.6706,  ..., 0.5765, 0.5412, 0.5020]],\n",
       "\n",
       "        [[1.0000, 1.0000, 1.0000,  ..., 0.9569, 0.9451, 0.9373],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.9490, 0.9373, 0.9294],\n",
       "         [0.9922, 1.0000, 0.4392,  ..., 0.9412, 0.9294, 0.9255],\n",
       "         ...,\n",
       "         [0.6902, 0.6824, 0.6824,  ..., 0.6039, 0.5961, 0.5922],\n",
       "         [0.6902, 0.6824, 0.6784,  ..., 0.5804, 0.5569, 0.5686],\n",
       "         [0.6784, 0.6824, 0.6745,  ..., 0.5922, 0.5608, 0.5373]],\n",
       "\n",
       "        [[1.0000, 1.0000, 1.0000,  ..., 0.9529, 0.9412, 0.9333],\n",
       "         [0.9843, 0.9843, 0.9843,  ..., 0.9451, 0.9333, 0.9216],\n",
       "         [0.9686, 0.9765, 0.6196,  ..., 0.9373, 0.9255, 0.9059],\n",
       "         ...,\n",
       "         [0.6706, 0.6627, 0.6627,  ..., 0.6078, 0.6000, 0.6000],\n",
       "         [0.6706, 0.6627, 0.6588,  ..., 0.5843, 0.5608, 0.5765],\n",
       "         [0.6588, 0.6627, 0.6549,  ..., 0.6000, 0.5647, 0.5451]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_application_operator(240, h = 'left', v = 'up', ratio = 2, rotation = 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models on Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epochs, batch_size, lr, reg, \n",
    "          transform_train=transform_train, \n",
    "          transform_test=transform_test, \n",
    "          log_every_n=50, \n",
    "          device = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Training a network\n",
    "    :param net: Network for training\n",
    "    :param epochs: Number of epochs in total.\n",
    "    :param batch_size: Batch size for training.\n",
    "    \"\"\"\n",
    "    print('==> Preparing data..')\n",
    "\n",
    "    best_acc = 0  # best test accuracy\n",
    "    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "    trainset = torchvision.datasets.CIFAR10(root='../00_data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='../00_data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.875, weight_decay=reg, nesterov=False)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs*0.5), int(epochs*0.75)], gamma=0.1)\n",
    "\n",
    "    global_steps = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        \"\"\"\n",
    "        Start the training code.\n",
    "        \"\"\"\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            global_steps += 1\n",
    "\n",
    "            if global_steps % log_every_n == 0:\n",
    "                end = time.time()\n",
    "                num_examples_per_second = log_every_n * batch_size / (end - start)\n",
    "                print(\"[Step=%d]\\tLoss=%.4f\\tacc=%.4f\\t%.1f examples/second\"\n",
    "                      % (global_steps, train_loss / (batch_idx + 1), (correct / total), num_examples_per_second))\n",
    "                start = time.time()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        \"\"\"\n",
    "        Start the testing code.\n",
    "        \"\"\"\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        num_val_steps = len(testloader)\n",
    "        val_acc = correct / total\n",
    "        print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Saving...\")\n",
    "            torch.save(net.state_dict(), \"pretrained_model.pt\")\n",
    "\n",
    "def test(net, device = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "\n",
    "    ])\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='../00_data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test accuracy=%.4f\" % (test_loss / (num_val_steps), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/codespace/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/codespace/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100.0%\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet161_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet161_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /home/codespace/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# ResNet, DenseNet and VGG\n",
    "\n",
    "# Load the pretrained model from pytorch - okay this is probably wrong. look into this\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "vgg = torchvision.models.vgg16(pretrained=True)\n",
    "densenet = torchvision.models.densenet161(pretrained=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/multiprocessing/queues.py\", line 244, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 428, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/storage.py\", line 297, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/storage.py\", line 330, in _share_fd_cpu_\n",
      "    return super()._share_fd_cpu_(*args, **kwargs)\n",
      "RuntimeError: unable to write to file </torch_12044_1703381552_1>: No space left on device (28)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 12050) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(resnet, \u001b[39m20\u001b[39;49m, batch_size, lr\u001b[39m=\u001b[39;49m\u001b[39m0.002\u001b[39;49m, reg\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[21], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, epochs, batch_size, lr, reg, transform_train, transform_test, log_every_n, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader):\n\u001b[0;32m---> 40\u001b[0m     inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39;49mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m     outputs \u001b[39m=\u001b[39m net(inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(previous_handler)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 12050) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit."
     ]
    }
   ],
   "source": [
    "train(resnet, 20, batch_size, lr=0.002, reg=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST CELLS - IGNORE STUFF BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path will be placed right-center\n",
      "Image saved at: ../00_data/test_images/banana-patched.jpeg\n"
     ]
    }
   ],
   "source": [
    "h = 'right'\n",
    "v = 'center'\n",
    "ratio = 3\n",
    "\n",
    "PATCH_PATH = '../00_data/patches/toaster_patch.png'\n",
    "src_img = '../00_data/test_images/banana.jpeg'\n",
    "save_path = '../00_data/test_images/banana-patched.jpeg'\n",
    "\n",
    "mode = {\n",
    "    'h': h,\n",
    "    'v': v\n",
    "}\n",
    "\n",
    "print(f\"Path will be placed {mode['h']}-{mode['v']}\")\n",
    "\n",
    "# map from mode value to a function that returns the correct value position\n",
    "mode_to_value = {\n",
    "    'left': lambda width, path: 0,\n",
    "    'center_h': lambda width, path: width // 2 - path.width // 2,\n",
    "    'center_v': lambda height, path: height // 2 - path.height // 2,\n",
    "    'right': lambda width, path: width - path.width,\n",
    "    'up': lambda height, path: 0,\n",
    "    'down': lambda height, path: height - path.height,\n",
    "}\n",
    "\n",
    "def get_position(mode, image, path):\n",
    "    x = mode_to_value[mode['h'] if mode['h'] != 'center' else 'center_h'](image.width, path)\n",
    "    y = mode_to_value[mode['v'] if mode['v'] != 'center' else 'center_v'](image.height, path)\n",
    "    return x, y\n",
    "\n",
    "patch_image = Image.open(PATCH_PATH)\n",
    "src_image = Image.open(src_img)\n",
    "\n",
    "patch_image = patch_image.resize((src_image.width // ratio, src_image.height // ratio), Image.Resampling.LANCZOS)\n",
    "position = get_position(mode, src_image, patch_image)\n",
    "\n",
    "src_image.paste(patch_image, position, mask = patch_image)\n",
    "src_image.save(save_path)\n",
    "\n",
    "print(f\"Image saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
